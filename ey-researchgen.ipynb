{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install --upgrade --force-reinstall fsspec==2024.10.0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-11T09:53:45.252847Z","iopub.execute_input":"2025-03-11T09:53:45.253068Z","iopub.status.idle":"2025-03-11T09:53:50.032718Z","shell.execute_reply.started":"2025-03-11T09:53:45.253019Z","shell.execute_reply":"2025-03-11T09:53:50.031788Z"}},"outputs":[{"name":"stdout","text":"Collecting fsspec==2024.10.0\n  Downloading fsspec-2024.10.0-py3-none-any.whl.metadata (11 kB)\nDownloading fsspec-2024.10.0-py3-none-any.whl (179 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.6/179.6 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: fsspec\n  Attempting uninstall: fsspec\n    Found existing installation: fsspec 2024.12.0\n    Uninstalling fsspec-2024.12.0:\n      Successfully uninstalled fsspec-2024.12.0\nSuccessfully installed fsspec-2024.10.0\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"pip install langchain_community langchain_groq fpdf flask flask_cors pyngrok","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-11T09:53:50.033766Z","iopub.execute_input":"2025-03-11T09:53:50.034077Z","iopub.status.idle":"2025-03-11T09:54:02.043423Z","shell.execute_reply.started":"2025-03-11T09:53:50.034021Z","shell.execute_reply":"2025-03-11T09:54:02.042556Z"}},"outputs":[{"name":"stdout","text":"Collecting langchain_community\n  Downloading langchain_community-0.3.19-py3-none-any.whl.metadata (2.4 kB)\nCollecting langchain_groq\n  Downloading langchain_groq-0.2.5-py3-none-any.whl.metadata (2.6 kB)\nCollecting fpdf\n  Downloading fpdf-1.7.2.tar.gz (39 kB)\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: flask in /usr/local/lib/python3.10/dist-packages (3.1.0)\nCollecting flask_cors\n  Downloading flask_cors-5.0.1-py3-none-any.whl.metadata (961 bytes)\nCollecting pyngrok\n  Downloading pyngrok-7.2.3-py3-none-any.whl.metadata (8.7 kB)\nCollecting langchain-core<1.0.0,>=0.3.41 (from langchain_community)\n  Downloading langchain_core-0.3.43-py3-none-any.whl.metadata (5.9 kB)\nCollecting langchain<1.0.0,>=0.3.20 (from langchain_community)\n  Downloading langchain-0.3.20-py3-none-any.whl.metadata (7.7 kB)\nRequirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (2.0.36)\nRequirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (2.32.3)\nRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (6.0.2)\nRequirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (3.11.12)\nRequirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (9.0.0)\nRequirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.6.7)\nCollecting pydantic-settings<3.0.0,>=2.4.0 (from langchain_community)\n  Downloading pydantic_settings-2.8.1-py3-none-any.whl.metadata (3.5 kB)\nRequirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.2.3)\nCollecting httpx-sse<1.0.0,>=0.4.0 (from langchain_community)\n  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\nRequirement already satisfied: numpy<3,>=1.26.2 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (1.26.4)\nCollecting groq<1,>=0.4.1 (from langchain_groq)\n  Downloading groq-0.18.0-py3-none-any.whl.metadata (14 kB)\nRequirement already satisfied: Werkzeug>=3.1 in /usr/local/lib/python3.10/dist-packages (from flask) (3.1.3)\nRequirement already satisfied: Jinja2>=3.1.2 in /usr/local/lib/python3.10/dist-packages (from flask) (3.1.4)\nRequirement already satisfied: itsdangerous>=2.2 in /usr/local/lib/python3.10/dist-packages (from flask) (2.2.0)\nRequirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.10/dist-packages (from flask) (8.1.7)\nRequirement already satisfied: blinker>=1.9 in /usr/local/lib/python3.10/dist-packages (from flask) (1.9.0)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.4.6)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.2)\nRequirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (5.0.1)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (25.1.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.1.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.2.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.18.3)\nRequirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.26.1)\nRequirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\nRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from groq<1,>=0.4.1->langchain_groq) (3.7.1)\nRequirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from groq<1,>=0.4.1->langchain_groq) (1.9.0)\nRequirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from groq<1,>=0.4.1->langchain_groq) (0.28.1)\nRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from groq<1,>=0.4.1->langchain_groq) (2.11.0a2)\nRequirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from groq<1,>=0.4.1->langchain_groq) (1.3.1)\nRequirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.10/dist-packages (from groq<1,>=0.4.1->langchain_groq) (4.12.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=3.1.2->flask) (3.0.2)\nCollecting langchain-text-splitters<1.0.0,>=0.3.6 (from langchain<1.0.0,>=0.3.20->langchain_community)\n  Downloading langchain_text_splitters-0.3.6-py3-none-any.whl.metadata (1.9 kB)\nCollecting async-timeout<6.0,>=4.0 (from aiohttp<4.0.0,>=3.8.3->langchain_community)\n  Downloading async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\nRequirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<1.0.0,>=0.3.41->langchain_community) (1.33)\nRequirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<1.0.0,>=0.3.41->langchain_community) (24.2)\nRequirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (3.10.12)\nRequirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (1.0.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy<3,>=1.26.2->langchain_community) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy<3,>=1.26.2->langchain_community) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy<3,>=1.26.2->langchain_community) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy<3,>=1.26.2->langchain_community) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy<3,>=1.26.2->langchain_community) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy<3,>=1.26.2->langchain_community) (2.4.1)\nCollecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain_community)\n  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (2025.1.31)\nRequirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.1.1)\nRequirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->groq<1,>=0.4.1->langchain_groq) (1.2.2)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain_groq) (1.0.7)\nRequirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain_groq) (0.14.0)\nRequirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.41->langchain_community) (3.0.0)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->groq<1,>=0.4.1->langchain_groq) (0.7.0)\nRequirement already satisfied: pydantic-core==2.29.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->groq<1,>=0.4.1->langchain_groq) (2.29.0)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.0.0)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<3,>=1.26.2->langchain_community) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<3,>=1.26.2->langchain_community) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy<3,>=1.26.2->langchain_community) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy<3,>=1.26.2->langchain_community) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy<3,>=1.26.2->langchain_community) (2024.2.0)\nDownloading langchain_community-0.3.19-py3-none-any.whl (2.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m30.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading langchain_groq-0.2.5-py3-none-any.whl (15 kB)\nDownloading flask_cors-5.0.1-py3-none-any.whl (11 kB)\nDownloading pyngrok-7.2.3-py3-none-any.whl (23 kB)\nDownloading groq-0.18.0-py3-none-any.whl (121 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.9/121.9 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\nDownloading langchain-0.3.20-py3-none-any.whl (1.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m45.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading langchain_core-0.3.43-py3-none-any.whl (415 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m415.4/415.4 kB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pydantic_settings-2.8.1-py3-none-any.whl (30 kB)\nDownloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\nDownloading langchain_text_splitters-0.3.6-py3-none-any.whl (31 kB)\nDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\nBuilding wheels for collected packages: fpdf\n  Building wheel for fpdf (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for fpdf: filename=fpdf-1.7.2-py2.py3-none-any.whl size=40704 sha256=924a19ebb8dddac90d5cbac666a04b1d55457e0b7fc79c2053363c392a4df808\n  Stored in directory: /root/.cache/pip/wheels/f9/95/ba/f418094659025eb9611f17cbcaf2334236bf39a0c3453ea455\nSuccessfully built fpdf\nInstalling collected packages: fpdf, python-dotenv, pyngrok, httpx-sse, async-timeout, pydantic-settings, groq, flask_cors, langchain-core, langchain-text-splitters, langchain_groq, langchain, langchain_community\n  Attempting uninstall: async-timeout\n    Found existing installation: async-timeout 5.0.1\n    Uninstalling async-timeout-5.0.1:\n      Successfully uninstalled async-timeout-5.0.1\n  Attempting uninstall: langchain-core\n    Found existing installation: langchain-core 0.3.25\n    Uninstalling langchain-core-0.3.25:\n      Successfully uninstalled langchain-core-0.3.25\n  Attempting uninstall: langchain-text-splitters\n    Found existing installation: langchain-text-splitters 0.3.3\n    Uninstalling langchain-text-splitters-0.3.3:\n      Successfully uninstalled langchain-text-splitters-0.3.3\n  Attempting uninstall: langchain\n    Found existing installation: langchain 0.3.12\n    Uninstalling langchain-0.3.12:\n      Successfully uninstalled langchain-0.3.12\nSuccessfully installed async-timeout-4.0.3 flask_cors-5.0.1 fpdf-1.7.2 groq-0.18.0 httpx-sse-0.4.0 langchain-0.3.20 langchain-core-0.3.43 langchain-text-splitters-0.3.6 langchain_community-0.3.19 langchain_groq-0.2.5 pydantic-settings-2.8.1 pyngrok-7.2.3 python-dotenv-1.0.1\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"pip install torch torchvision torchaudio diffusers safetensors huggingface_hub pillow cloudinary google-generativeai","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-11T09:54:02.045121Z","iopub.execute_input":"2025-03-11T09:54:02.045353Z","iopub.status.idle":"2025-03-11T09:54:05.820506Z","shell.execute_reply.started":"2025-03-11T09:54:02.045333Z","shell.execute_reply":"2025-03-11T09:54:05.819656Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\nRequirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.1+cu121)\nRequirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\nRequirement already satisfied: diffusers in /usr/local/lib/python3.10/dist-packages (0.31.0)\nRequirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (0.4.5)\nRequirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.29.0)\nRequirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (11.0.0)\nCollecting cloudinary\n  Downloading cloudinary-1.42.2-py3-none-any.whl.metadata (7.9 kB)\nRequirement already satisfied: google-generativeai in /usr/local/lib/python3.10/dist-packages (0.8.3)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.17.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\nRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.10/dist-packages (from diffusers) (8.5.0)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from diffusers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from diffusers) (2.32.3)\nRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (6.0.2)\nRequirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.67.1)\nRequirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from cloudinary) (1.17.0)\nRequirement already satisfied: urllib3>=1.26.5 in /usr/local/lib/python3.10/dist-packages (from cloudinary) (2.3.0)\nRequirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from cloudinary) (2025.1.31)\nRequirement already satisfied: google-ai-generativelanguage==0.6.10 in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (0.6.10)\nRequirement already satisfied: google-api-core in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (1.34.1)\nRequirement already satisfied: google-api-python-client in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (2.155.0)\nRequirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (2.27.0)\nRequirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (3.20.3)\nRequirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (2.11.0a2)\nRequirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.10/dist-packages (from google-ai-generativelanguage==0.6.10->google-generativeai) (1.25.0)\nRequirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core->google-generativeai) (1.66.0)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-generativeai) (5.5.0)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-generativeai) (0.4.1)\nRequirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-generativeai) (4.9)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers) (3.10)\nRequirement already satisfied: httplib2<1.dev0,>=0.19.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->google-generativeai) (0.22.0)\nRequirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->google-generativeai) (0.2.0)\nRequirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->google-generativeai) (4.1.1)\nRequirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata->diffusers) (3.21.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (2.4.1)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic->google-generativeai) (0.7.0)\nRequirement already satisfied: pydantic-core==2.29.0 in /usr/local/lib/python3.10/dist-packages (from pydantic->google-generativeai) (2.29.0)\nRequirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.10->google-generativeai) (1.68.1)\nRequirement already satisfied: grpcio-status<2.0dev,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.10->google-generativeai) (1.48.2)\nRequirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.10/dist-packages (from httplib2<1.dev0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.0)\nRequirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.1)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->torchvision) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->torchvision) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->torchvision) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->torchvision) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->torchvision) (2024.2.0)\nDownloading cloudinary-1.42.2-py3-none-any.whl (146 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.0/147.0 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: cloudinary\nSuccessfully installed cloudinary-1.42.2\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"pip install --upgrade fsspec==2024.10.0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-11T09:54:05.821819Z","iopub.execute_input":"2025-03-11T09:54:05.822065Z","iopub.status.idle":"2025-03-11T09:54:09.244916Z","shell.execute_reply.started":"2025-03-11T09:54:05.822044Z","shell.execute_reply":"2025-03-11T09:54:09.243931Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: fsspec==2024.10.0 in /usr/local/lib/python3.10/dist-packages (2024.10.0)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"from pyngrok import ngrok\nngrok.set_auth_token(\"2rXNHkOEJNhfdg5d7eOFRycQW1m_7p1CvKr2cZV3c1zWBjLs8\")\nurl = ngrok.connect(8000)\nprint(url)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-11T09:54:09.245938Z","iopub.execute_input":"2025-03-11T09:54:09.246294Z","iopub.status.idle":"2025-03-11T09:54:12.635055Z","shell.execute_reply.started":"2025-03-11T09:54:09.246259Z","shell.execute_reply":"2025-03-11T09:54:12.634373Z"}},"outputs":[{"name":"stdout","text":"NgrokTunnel: \"https://0c5b-34-138-178-194.ngrok-free.app\" -> \"http://localhost:8000\"                \n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"import os\nimport json\nimport torch\nimport base64\nimport time\nimport cloudinary\nimport cloudinary.uploader\nimport numpy as np\nimport tempfile\nimport multiprocessing\nimport shutil\nimport logging\nfrom io import BytesIO\nfrom PIL import Image, ImageDraw, ImageFont\nfrom pydantic import BaseModel, TypeAdapter\nfrom diffusers import (\n    AnimateDiffPipeline, \n    MotionAdapter, \n    EulerDiscreteScheduler, \n    StableDiffusionPipeline\n)\nfrom diffusers.utils import export_to_gif\nfrom safetensors.torch import load_file\nfrom huggingface_hub import hf_hub_download\nimport torchaudio\nfrom moviepy.editor import (\n    AudioFileClip, \n    TextClip ,\n    ImageSequenceClip, \n    VideoFileClip, \n    concatenate_videoclips, \n    vfx\n)\nfrom google import genai\nfrom langchain_community.document_loaders import PyPDFLoader\n\n# ----------------- Configure Logging to File -----------------\nlogging.basicConfig(\n    filename='comic_debug.log',\n    level=logging.DEBUG,\n    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n)\n# ----------------- Model & Pipeline Setup -----------------\ndevice = \"cuda\"\ndtype = torch.float16\n\nstep = 4  # Options: [1,2,4,8]\nrepo = \"ByteDance/AnimateDiff-Lightning\"\nckpt = f\"animatediff_lightning_{step}step_diffusers.safetensors\"\nbase = \"emilianJR/epiCRealism\"  # Choose your favorite base model.\n\nadapter = MotionAdapter().to(device, dtype)\nadapter.load_state_dict(load_file(hf_hub_download(repo, ckpt), device=device))\npipe = AnimateDiffPipeline.from_pretrained(base, motion_adapter=adapter, torch_dtype=dtype).to(device)\npipe.scheduler = EulerDiscreteScheduler.from_config(pipe.scheduler.config, timestep_spacing=\"trailing\", beta_schedule=\"linear\")\n\n# Cloudinary configuration\ncloudinary.config(\n    cloud_name=\"dfx1wn6l4\",\n    api_key=\"489759552276462\",\n    api_secret=\"j0pCgqMZR8LS0x01Wil6ypNRIgM\"\n)\n\n# Load the Silero TTS model\ndevice = torch.device(\"cuda\")  \nmodel, _ = torch.hub.load(repo_or_dir=\"snakers4/silero-models\",\n                            model=\"silero_tts\",\n                            language=\"en\",\n                            speaker=\"v3_en\")\nmodel.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-11T09:54:12.635910Z","iopub.execute_input":"2025-03-11T09:54:12.636170Z","iopub.status.idle":"2025-03-11T09:55:25.151619Z","shell.execute_reply.started":"2025-03-11T09:54:12.636145Z","shell.execute_reply":"2025-03-11T09:55:25.150619Z"}},"outputs":[{"name":"stderr","text":"The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ffa44c93e0bd4390b054c013d87b426a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"(…)ff_lightning_4step_diffusers.safetensors:   0%|          | 0.00/908M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"17b5e6fb3b1f4356808febec4f0d639e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model_index.json:   0%|          | 0.00/579 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c7f16509a50746798a8e2ff52a8899c3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 15 files:   0%|          | 0/15 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"67895ca01fc64fe7973120738587ba50"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/520 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f4ac94c7548f47f085f034b484b35e5a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"scheduler_config.json:   0%|          | 0.00/528 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9c3524d44e2148d39ad6d29ac8bdf25c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/525k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0078b3760aa74dababdbc69df112025d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.22G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c2020f012c4d4e12983b0a7e4b8e92cd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/4.58k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b4f85489288140beba7ec3dc3601e4ac"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/472 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"36536ca5cc5640ee9002f9f1481f77e8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9da17800cd0e4057a34640dadb07747d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/492M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"54f690fbd3b1420b8ac1f8344e97196d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/737 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"01a8b72eff0043a4a83c83661163be0d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.06M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d2e2cbc8ad734bd0a1aa2c1c9878b8a6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/577 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"79d480b347b24ab1a20d53d35b0b8195"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.58k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"220812201f55449b8af4f2e472bbb198"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"diffusion_pytorch_model.safetensors:   0%|          | 0.00/3.44G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bd834fb08de44cff923cd67de3112bc4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"diffusion_pytorch_model.safetensors:   0%|          | 0.00/335M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"37e4dec1ec9d42a8af20faebcbfa2c1e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading pipeline components...:   0%|          | 0/6 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"13bd7d30eb464b0f8e888f35b1f8f7a6"}},"metadata":{}},{"name":"stderr","text":"Downloading: \"https://github.com/snakers4/silero-models/zipball/master\" to /root/.cache/torch/hub/master.zip\n100%|██████████| 54.5M/54.5M [00:02<00:00, 21.9MB/s]\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"\n\n# ----------------- Core Functions -----------------\n\ndef create_audio_video(script, video_prompt, output_filename=\"output_video.mp4\", sample_rate=48000, fps=24, mode=1):\n    # Generate audio\n    with torch.no_grad():\n        audio = model.apply_tts(text=script, speaker=\"en_11\", sample_rate=sample_rate)\n\n    # Create a temporary audio file\n    with tempfile.NamedTemporaryFile(delete=False, suffix=\".wav\") as temp_audio:\n        audio_file = temp_audio.name  # Store the file path\n        torchaudio.save(audio_file, audio.unsqueeze(0).cpu(), sample_rate)\n\n    # Load the audio into MoviePy\n    audio_clip = AudioFileClip(audio_file)\n\n    # Generate video frames using a pre-defined pipeline\n    output = pipe(prompt=video_prompt + \" NOTE! The video should be in a loop\", guidance_scale=1.0, num_inference_steps=4)\n\n    # Convert PIL images to NumPy arrays\n    frames = [np.array(frame.convert(\"RGB\")) for frame in output.frames[0]]\n\n    # Create video clip from frames\n    video_clip = ImageSequenceClip(frames, fps=fps)\n\n    if mode == 0:\n        # Loop the video to match the audio duration\n        final_clip = video_clip.loop(duration=audio_clip.duration)\n    elif mode == 1:\n        # Slow down the video to match the audio duration\n        speed_factor = video_clip.duration / audio_clip.duration  # Compute the slow-down factor\n        final_clip = video_clip.fx(vfx.speedx, factor=speed_factor).set_duration(audio_clip.duration)\n    else:\n        raise ValueError(\"Invalid mode. Use 0 for looping, 1 for slowing down.\")\n\n    # Set the audio for the video clip\n    final_clip = final_clip.set_audio(audio_clip)\n\n    # Write the final video to an MP4 file\n    final_clip.write_videofile(output_filename, codec=\"libx264\", audio_codec=\"aac\")\n\n    # Clean up temporary audio file\n    os.remove(audio_file)\n\ndef process_data(data, mode=0):\n    output_files = []\n    \n    # Loop over the input data and create individual video files.\n    for i, item in enumerate(data):\n        output_filename = f\"output_{i}.mp4\"\n        create_audio_video(\n            script=item[\"script\"], \n            video_prompt=item[\"video_prompt\"], \n            output_filename=output_filename,\n            mode=mode\n        )\n        output_files.append(output_filename)\n    \n    # Load all individual videos as clips.\n    clips = [VideoFileClip(os.path.join(os.getcwd(), fname)) for fname in output_files]\n    \n    # Concatenate all clips into one final video.\n    final_clip = concatenate_videoclips(clips, method=\"compose\")\n    final_clip.write_videofile(\"final_output.mp4\", codec=\"libx264\", audio_codec=\"aac\")\n    \n    # Close all clips.\n    final_clip.close()\n    for clip in clips:\n        clip.close()\n    \n    # Remove the individual video files.\n    for fname in output_files:\n        os.remove(fname)\n\ndef generate_video_from_paper(pdf_path):\n    loader = PyPDFLoader(pdf_path)\n    docs = loader.load()\n        \n    full_context = \"\"\n        \n    for i, doc in enumerate(docs):\n        full_context += f\"==========================page_{i}============================\"\n        full_context += doc.page_content\n        \n    # The prompt for generating video script and prompts.\n    prompt = \"\"\"\n    Your task is to generate a **video script** and **video prompt** based on the content provided below. The goal is to summarize the entire content in a **friendly yet formal** manner while ensuring that the visuals are engaging and highly descriptive.\n    \n    ### Instructions:\n    - Summarize the entire content in exactly **6 key points**.\n    - Each key point should contain:\n      1. **Script:** A concise, easy-to-understand explanation in **1-2 lines only**.\n      2. **Video Prompt:** A **highly descriptive** visual scene that best represents the script’s message.\n    - The video prompt should be vivid, creative, and engaging, ensuring **high-quality graphics** in the final video.\n    \n    ### Important Language Guidelines:\n    - **Do not use short forms or abbreviations.** Write everything in full words.\n    - **Avoid using brackets, symbols, or special characters.** Just use natural, everyday English.\n    - **Replace technical jargon with simple explanations.** Assume the audience has no prior knowledge.\n    - **For example instead of AI use Artifical intelligence**\n    - **Do not use any complex formatting or numbering systems.** Write everything as clear and natural sentences.\n    - **ENSURE TO HAVE INTRODUCTION AT START AND CONCLUSION AT END**\n    - **FOR VIDEO PROMPTS TRY TO MAKE THEM SCI-FI / FUTURISTIC**\n    \n    ### Response Format (JSON Structure):\n    \n    {\n        \"data\": [\n            {\n                \"script\": \"Welcome to another episode of Research GEN one Minute Papers!\",\n                \"video_prompt\": \"A futuristic artificial intelligence robot greeting the audience with a holographic screen displaying 'Research GEN'.\"\n            },\n            {\n                \"script\": \"Graph Neural Networks are a new technology in artificial intelligence that work similarly to how the brain's neurons communicate.\",\n                \"video_prompt\": \"A dynamic three-dimensional animation of a network where connections light up like brain neurons sending signals.\"\n            },\n            ...\n        ]\n    }\n    \n    Now, generate the script and video prompts based on the following content:\n    \n    \"\"\"\n    prompt += f\"\\n\\n{full_context}\\n\\n\"\n    \n    # Initialize Google Gemini client\n    client = genai.Client(api_key=\"AIzaSyAWi_dcty_RNWp0KjTJbIFcy8atNDsyaaw\")\n    \n    # Generate content with the correct schema format\n    response = client.models.generate_content(\n        model='gemini-2.0-flash',\n        contents=prompt,\n        config={\n            'response_mime_type': 'application/json',\n        },\n    )\n    # Parse JSON response\n    res = json.loads(response.candidates[0].content.parts[0].text)\n    process_data(res['data'], mode=0)\n\n# ----------------- Image Generation & Document Functions -----------------\n\nmodel_id = \"emilianJR/epiCRealism\"\nimage_pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\nimage_pipe = image_pipe.to(\"cuda\")\n\ndef generate_image(prompts):\n    # Append a sci-fi touch to each prompt.\n    for prompt in prompts:\n        prompt += \" add a scifi touch to the image\"\n    return image_pipe(prompts, num_inference_steps=30).images\n\ndef create_document_from_paper(file_path):\n    loader = PyPDFLoader(file_path)\n    docs = loader.load()\n    \n    full_context = \"\"\n    \n    for i, doc in enumerate(docs):\n        full_context += f\"==========================page_{i}============================\"\n        full_context += doc.page_content\n    \n    from google import genai\n    client = genai.Client(api_key=\"AIzaSyAWi_dcty_RNWp0KjTJbIFcy8atNDsyaaw\")\n    response = client.models.generate_content(\n        model=\"gemini-2.0-flash\", \n        contents=f\"\"\"\n    \n    below is the context of entire paper your job is to summerise the entire paper in exactly 10 brief points each of 11-12 starting from introduction to conclusion \n    \n    the format should be \n    \n    sr no . Title : text ( 10-11 lines)\n    eg : 1.  **Introduction to Graph Neural Networks (GNNs):** The paper begins by....\n    \n    context : \n    \n    {full_context}\n    \n    \"\"\"\n    )\n    \n    from langchain_groq import ChatGroq\n    from google.colab import userdata\n    from typing import Optional\n    from langchain_core.pydantic_v1 import BaseModel, Field\n    from typing import List\n    \n    llm = ChatGroq(\n        model=\"llama-3.3-70b-versatile\", api_key=\"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\"\n    )\n    \n    class Content():\n        title: str\n        text: str\n        image_prompt: str\n    \n    class data():\n        title: str\n        data: List[Content]\n        title_image_prompt: str\n    \n    structured_llm = llm.with_structured_output(data)\n    prompt = f\"\"\"\n    For the following context : {response.text},\n    \n    Please generate structured content in JSON format with 10 detailed sections. Each section must include:  \n    - A \"title\" (the name of the sub-topic)  \n    - A \"text\" (a long, around 15-16 lines detailed explanation about the sub-topic, explain each point in high detail as you are explaining to a fresher to this)  \n    - An \"image_prompt\" (a simple, layman-friendly description of an image related to the sub-topic)  \n    \n    The final structured output should be in the following format:  \n    \n    {{  \n      \"title\": \"Graph Neural Networks\",  \n      \"data\": [  \n        {{  \n          \"title\": \"<Sub-topic 1>\",  \n          \"text\": \"<Detailed explanation of Sub-topic 1>\",  \n          \"image_prompt\": \"<Simple description of an image related to Sub-topic 1>\"  \n        }},  \n        {{  \n          \"title\": \"<Sub-topic 2>\",  \n          \"text\": \"<Detailed explanation of Sub-topic 2>\",  \n          \"image_prompt\": \"<Simple description of an image related to Sub-topic 2>\"  \n        }},  \n        ... (total 10 such sections)  \n      ],  \n      \"title_image_prompt\": \"A simple and easy-to-understand image description representing the overall topic, Graph Neural Networks.\"  \n    }}  \n    \n    ### Additional Instructions:  \n    - **Ensure there are exactly 10 sections** covering various aspects of Graph Neural Networks.  \n    - **Keep the text detailed and informative**, explaining concepts in-depth.  \n    - **Keep image descriptions simple and easy to interpret** (avoid complex technical terms).  \n    \n    \"\"\"\n    \n    final_ans = structured_llm.invoke(prompt)\n    time.sleep(2)  # slight delay\n    from fpdf import FPDF\n    \n    def clean_text(text):\n        return text.encode('latin-1', 'ignore').decode('latin-1')\n    \n    pdf = FPDF()\n    pdf.set_auto_page_break(auto=True, margin=15)\n    \n    data_list = final_ans['data']\n    if 'title_image_prompt' not in final_ans:\n        final_ans['title_image_prompt'] = 'futuristic robot in a data center'\n    \n    title_image = generate_image([final_ans['title_image_prompt']])[0]\n    title_image_path = os.path.join(os.getcwd(), \"title_image.jpg\")\n    title_image.resize((1150, 600)).save(title_image_path)\n    \n    pdf.add_page()\n    pdf.image(title_image_path, x=10, w=190)\n    pdf.ln(10)\n    \n    pdf.set_text_color(0, 0, 128)\n    pdf.set_font(\"Arial\", \"B\", 24)\n    pdf.cell(200, 20, \"ResearchGen\", ln=True, align=\"C\")\n    pdf.ln(10)\n    \n    pdf.set_text_color(0, 0, 0)\n    pdf.set_font(\"Arial\", \"B\", 20)\n    pdf.multi_cell(0, 15, clean_text(final_ans['title']), align=\"C\")\n    pdf.ln(20)\n    \n    pdf.set_font(\"Arial\", \"I\", 14)\n    pdf.cell(200, 10, \"Generated Research Document\", ln=True, align=\"C\")\n    \n    prompts = [entry['image_prompt'] + \" in a sci-fi tone\" for entry in data_list]\n    images = generate_image(prompts)\n    \n    for i, entry in enumerate(data_list):\n        pdf.add_page()\n        pdf.set_text_color(128, 0, 128)\n        pdf.set_font(\"Arial\", \"B\", 16)\n        pdf.cell(200, 10, clean_text(entry['title']), ln=True, align=\"C\")\n        pdf.ln(10)\n    \n        image = images[i].resize((1150, 600))\n        image_path = os.path.join(os.getcwd(), f\"generated_image_{i}.jpg\")\n        image.save(image_path)\n    \n        pdf.image(image_path, x=10, w=190)\n        pdf.ln(10)\n    \n        pdf.set_text_color(0, 0, 0)\n        pdf.set_font(\"Arial\", size=12)\n        pdf.multi_cell(0, 10, clean_text(entry['text']))\n    \n    output_path = os.path.join(os.getcwd(), \"research.pdf\")\n    pdf.output(output_path)\n\n\n\n\ndef generate_podcast_from_pdf(pdf_path):\n    loader = PyPDFLoader(pdf_path)\n    docs = loader.load()\n            \n    full_context = \"\"\n            \n    for i, doc in enumerate(docs):\n        full_context += f\"==========================page_{i}============================\"\n        full_context += doc.page_content\n            \n        # The prompt for generating video script and prompts.\n    prompt = \"\"\"\n    Create a script for a podcast featuring two individuals.\n    \n    - Person 1 will ask engaging and curious questions.\n    - Person 2 will explain concepts based on a provided context.\n    \n    ### Structure of the Podcast:\n    1. Person 2 starts by introducing the topic in a way that is engaging and easy to understand.\n    2. Person 1 asks questions based on curiosity and interest.\n    3. Person 2 responds with explanations that are informative yet accessible to a general audience.\n    4. The conversation should feel natural, dynamic, and engaging.\n    \n    ### Key Requirements:\n    - The podcast should be highly engaging and interesting to listen to.\n    - Use clear and simple language, avoiding technical jargon.\n    - Do not use abbreviations or short forms (e.g., use \"Artificial Intelligence\" instead of \"AI\").\n    - Do not use special characters, brackets, or symbols—write in natural, everyday English.\n    - Assume the audience has no prior knowledge and explain everything in simple terms.\n    \n    ### Output Format (JSON):\n    Ensure the output follows this format with exactly 40 exchanges in the `data` array:\n    \n    {\n      \"data\": [\n        {\n          \"speaker\": \"Person 2\",\n          \"script\": \"So, have you heard about the latest research on Graph Neural Networks?\"\n        },\n        {\n          \"speaker\": \"Person 1\",\n          \"script\": \"No, I have not. What is it? It sounds really interesting.\"\n        },\n        {\n          \"speaker\": \"Person 2\",\n          \"script\": \"It is a new type of network designed to analyze and find patterns in complex relationships that were difficult to understand before.\"\n        },\n        {\n          \"speaker\": \"Person 1\",\n          \"script\": \"That sounds fascinating! How does it actually work?\"\n        },\n        {\n          \"speaker\": \"Person 2\",\n          \"script\": \"Well, it combines ideas from both graphs and neural networks to process information in a unique way.\"\n        }\n        ...\n      ]\n    }\n    \n    - The data array must contain exactly 40 dialogue exchanges (20 from each speaker).\n    - The conversation should build naturally, starting with an introduction, followed by explanations and deep-dive questions.\n    - End with a strong conclusion, summarizing key points and leaving the listener intrigued.\n    \n    \n    note use commas extensively wehrever required \n    \n    now based on the below provided material provide a podcast\n    \"\"\"\n    \n        \n        \n    prompt += f\"\\n\\n{full_context}\\n\\n\"\n        \n        # Initialize Google Gemini client\n    client = genai.Client(api_key=\"AIzaSyAWi_dcty_RNWp0KjTJbIFcy8atNDsyaaw\")\n        \n        # Generate content with the correct schema format\n    response = client.models.generate_content(\n            model='gemini-2.0-flash',\n            contents=prompt,\n            config={\n                'response_mime_type': 'application/json',\n            },\n    )\n        # Parse JSON response\n    res = json.loads(response.candidates[0].content.parts[0].text)\n    \n    from IPython.display import Audio\n    from tqdm import tqdm  # Import tqdm for progress bar\n    \n    audios = []  # List to store generated audio clips\n    \n    \n    # Generate audio with tqdm progress bar\n    for direction in tqdm(res['data'], desc=\"Generating Audio\"):\n        speaker_type = \"en_11\"\n        if direction['speaker'] == \"Person 2\":\n            speaker_type = \"en_15\"\n        \n        # Generate audio for each script\n        audio_clip = model.apply_tts(text=direction['script'], speaker=speaker_type, sample_rate=48000)\n        audios.append(audio_clip)\n    \n    # Concatenate all generated audio tensors\n    tts_audio = torch.cat(audios, dim=-1)  # Concatenate along the time axis\n    \n    # Convert to NumPy for playback\n    tts_audio = tts_audio.detach().cpu().numpy()\n\n    return res ,  tts_audio\n\n\n# def create_comic_page(images, captions, images_per_row=3, bg_color=(255, 255, 255)):  \n#     \"\"\"  \n#     Combine images into a comic page with captions in a matrix format.  \n#     \"\"\"  \n#     # Ensure images and captions match  \n#     if len(images) != len(captions):  \n#         raise ValueError(\"The number of images and captions must be the same.\")  \n\n#     # Load images  \n#     images = [Image.open(img).convert(\"RGB\") for img in images]  \n\n#     # Optionally resize images to a fixed width  \n#     target_width = 300  \n#     resized_images = []  \n#     for img in images:  \n#         aspect_ratio = img.height / img.width  \n#         new_height = int(target_width * aspect_ratio)  \n#         resized_images.append(img.resize((target_width, new_height), Image.LANCZOS))  \n#     images = resized_images  \n\n#     # Calculate dimensions for the comic page  \n#     num_images = len(images)  \n#     num_rows = (num_images + images_per_row - 1) // images_per_row  \n#     max_height = max(img.height for img in images)  \n#     total_width = target_width * images_per_row  \n\n#     comic_height = max_height * num_rows  \n#     comic_img = Image.new(\"RGB\", (total_width, comic_height), color=bg_color)  \n#     draw = ImageDraw.Draw(comic_img)  \n\n#     # Attempt to load a TTF font; fallback if not available  \n#     try:  \n#         font = ImageFont.truetype(\"arial.ttf\", 14)  \n#     except Exception:  \n#         font = ImageFont.load_default()  \n\n#     # Paste images and captions  \n#     for index, (img, caption) in enumerate(zip(images, captions)):  \n#         row = index // images_per_row  \n#         col = index % images_per_row  \n#         x_offset = col * target_width  \n#         y_offset = row * max_height  \n\n#         # Paste image  \n#         comic_img.paste(img, (x_offset, y_offset))  \n\n#         # Draw caption on the image  \n#         text_width, text_height = draw.textsize(caption, font=font)  \n#         text_x = x_offset + (target_width - text_width) // 2  \n#         text_y = y_offset + 10  # Adjust caption position as needed  \n#         draw.text((text_x, text_y), caption, fill=\"black\", font=font)  \n\n#     return comic_img\ndef generate_image_podcast(prompts):  \n    for i in range(len(prompts)):  \n        prompts[i] += \" add a sci-fi touch to the image\"  \n    return image_pipe(prompts, num_inference_steps=30).images  \n    \ndef get_best_font(size=48):  \n    possible_fonts = [\"ComicSansMS-Bold.ttf\", \"comicbd.ttf\", \"arialbd.ttf\"]  \n    for font in possible_fonts:  \n        if os.path.exists(font):  \n            return ImageFont.truetype(font, size)  \n    return ImageFont.load_default() \n\ndef add_speech_bubble(img, text, position):  \n    draw = ImageDraw.Draw(img)  \n    font = get_best_font(size=48)  \n    wrapped_text = textwrap.fill(text, width=20)  \n\n    # Calculate text dimensions  \n    text_size = draw.textbbox((0, 0), wrapped_text, font=font)  \n    bubble_width = text_size[2] - text_size[0] + 40  \n    bubble_height = text_size[3] - text_size[1] + 40  \n\n    # Draw the speech bubble  \n    x, y = position  \n    draw.rounded_rectangle((x, y, x + bubble_width, y + bubble_height), radius=20, fill=(255, 255, 255), outline=\"black\", width=4)  \n    draw.text((x + 10, y + 10), wrapped_text, font=font, fill=(0, 0, 0))  \n    comic_page_path = os.path.join(os.getcwd(), \"tp.jpg\")\n    img.save(comic_page_path)\n    return img  \n\ndef create_comic_page(images, captions):  \n    comic_panels = []  \n\n    for i, img in enumerate(images):  \n        dialogue = captions[i]  \n        img = add_speech_bubble(img, dialogue, (20, 20))  \n        comic_panels.append(img)  \n\n    num_images = len(comic_panels)  \n    cols = min(max(num_images // 3, 1), 4)  # 3 images per row  \n    rows = math.ceil(num_images / cols)  \n    comic_grid = Image.new(\"RGB\", (cols * 512, rows * 512), \"white\")  \n\n    for i, panel in enumerate(comic_panels):  \n        x_offset, y_offset = (i % cols) * 512, (i // cols) * 512  \n        comic_grid.paste(panel, (x_offset, y_offset))  \n\n    return comic_grid\n\ndef generate_comic_from_pdf(pdf_path):\n    \"\"\"\n    1. Load the research paper from PDF.\n    2. Build a prompt for Gemini to produce a JSON structure with 'panels'.\n    3. Generate each panel image individually.\n    4. Compose them into a single \"comic_page.jpg\".\n    5. (Optional) Upload to Cloudinary if desired.\n    6. Return the path to the final comic image (or the Cloudinary URL).\n    \"\"\"\n    logging.info(\"generate_comic_from_pdf started for: %s\", pdf_path)\n\n    # 1. Read PDF content\n    loader = PyPDFLoader(pdf_path)\n    docs = loader.load()\n    full_context = \"\"\n    for i, doc in enumerate(docs):\n        full_context += f\"=== Page {i} ===\\n{doc.page_content}\\n\"\n\n    logging.debug(\"PDF loaded. Full context length: %d chars\", len(full_context))\n\n    # 2. Build the Gemini prompt\n    prompt = f\"\"\"\nYour task is to transform the provided research paper content into a comic page format.\nFor each key sentence or section, generate a fun, informative, and engaging caption \nalong with a vivid comic style image prompt. \nThe caption should be humorous, interesting, and directly related to the paper's content.\nOutput the result in JSON format with the following structure:\n{{\n  \"panels\": [\n    {{\n      \"caption\": \"A funny, informative sentence summarizing a key part of the paper.\",\n      \"image_prompt\": \"A detailed description for a comic panel image.\"\n    }},\n    ...\n  ]\n}}\nOnly include panels for the key parts that capture the essence of the paper.\nUse the provided full context below.\nGenerate 10-12 panels.\n\nFull context:\n{full_context}\n\"\"\"\n    logging.info(\"Gemini prompt built. First 500 chars: %s\", prompt[:500])\n\n    # 3. Send prompt to Gemini\n    client = genai.Client(api_key=\"AIzaSyAWi_dcty_RNWp0KjTJbIFcy8atNDsyaaw\")\n    response = client.models.generate_content(\n        model='gemini-2.0-flash',\n        contents=prompt,\n        config={'response_mime_type': 'application/json'},\n    )\n    \n    # Extract the JSON from Gemini\n    response_text = response.candidates[0].content.parts[0].text\n    logging.info(\"Response from Gemini (first 500 chars): %s\", response_text[:500])\n\n    comic_data = json.loads(response_text)\n    panels = comic_data.get(\"panels\", [])\n    if not panels:\n        raise ValueError(\"No 'panels' found in Gemini response. Possibly empty or invalid JSON.\")\n\n    logging.info(\"Number of panels: %d\", len(panels))\n\n    # Extract captions and image prompts\n    captions = [p.get(\"caption\", \"No caption\") for p in panels]\n    image_prompts = [p.get(\"image_prompt\", \"No prompt\") for p in panels]\n\n    # 4. Generate each panel image\n    images_dir = os.path.join(os.getcwd(), \"comic_images\")\n    os.makedirs(images_dir, exist_ok=True)\n    logging.info(\"Generating images. Storing individual panels in: %s\", images_dir)\n\n    generated_images = []\n    for idx, prompt_text in enumerate(image_prompts):\n        logging.info(\"Generating image %d with prompt: %s\", idx+1, prompt_text)\n        try:\n            imgs = generate_image_podcast([prompt_text])\n            if not imgs:\n                logging.error(\"No image returned for panel %d. Using a blank image instead.\", idx+1)\n                blank = Image.new(\"RGB\", (400, 400), color=(255, 255, 255))\n                generated_images.append(blank)\n            else:\n                final_img = imgs[0].resize((512, 512))  # Resize to fit comic  \n                # generated_images.append(final_img)  \n                panel_path = os.path.join(images_dir, f\"panel_{idx+1}.jpg\")\n                final_img.save(panel_path)\n                logging.info(\"Saved panel %d to %s\", idx+1, panel_path)\n                generated_images.append(final_img)\n        except Exception as e:\n            logging.exception(\"Error generating image for panel %d\", idx+1)\n            # Fallback to a blank image in case of error\n            blank = Image.new(\"RGB\", (400, 400), color=(255, 255, 255))\n            generated_images.append(blank)\n\n    # 5. Compose final comic page\n    comic_page = create_comic_page(generated_images, captions)\n    comic_page_path = os.path.join(os.getcwd(), \"comic_page.jpg\")\n    comic_page.save(comic_page_path)\n    logging.info(\"Final comic page saved to: %s\", comic_page_path)\n\n    # 6. (Optional) Upload to Cloudinary\n    # Comment out if you do not need to upload\n    # cloudinary.config(\n    #     cloud_name=\"dfx1wn6l4\",\n    #     api_key=\"489759552276462\",\n    #     api_secret=\"j0pCgqMZR8LS0x01Wil6ypNRIgM\"\n    # )\n    upload_result = cloudinary.uploader.upload(comic_page_path)\n    comic_url = upload_result.get(\"secure_url\", \"\")\n    logging.info(\"Comic page uploaded to Cloudinary. URL: %s\", comic_url)\n    return comic_url\n\n    # If you do NOT want Cloudinary, just return the local path:\n    # return comic_page_path\n\n# ----------------- Flask App with CORS -----------------\n\nfrom flask import Flask, request, send_file, jsonify\nfrom flask_cors import CORS\nimport soundfile as sf\n\napp = Flask(__name__)\nCORS(app, resources={r\"/*\": {\"origins\": \"*\"}}) \n\n@app.route(\"/generate_video\", methods=[\"POST\"])\ndef generate_video_endpoint():\n    # Get the uploaded file from the request\n    file = request.files.get(\"file\")\n    if file is None:\n        return jsonify({\"error\": \"No file provided\"}), 400\n\n    # Save the uploaded PDF file to a temporary location\n    with tempfile.NamedTemporaryFile(delete=False, suffix=\".pdf\") as tmp:\n        temp_pdf_path = tmp.name\n        file.save(tmp)\n    \n    try:\n        # Call the video generation function\n        generate_video_from_paper(temp_pdf_path)\n    except Exception as e:\n        return jsonify({\"error\": \"Error generating video\", \"details\": str(e)}), 500\n    finally:\n        os.remove(temp_pdf_path)\n    \n    video_file_path = \"/kaggle/working/final_output.mp4\"\n    # Check if the video file exists before sending\n    if not os.path.exists(video_file_path):\n        return jsonify({\"error\": \"Video file not found\", \"details\": f\"{video_file_path} does not exist\"}), 500\n\n    return send_file(\"/kaggle/working/final_output.mp4\", as_attachment=True, download_name=\"final_output.mp4\")\n\n@app.route(\"/generate_pdf\", methods=[\"POST\"])\ndef generate_pdf_endpoint():\n\n    file = request.files.get(\"file\")\n    if file is None:\n        return jsonify({\"error\": \"No file provided\"}), 400\n\n    # Save the uploaded PDF file to a temporary location\n    with tempfile.NamedTemporaryFile(delete=False, suffix=\".pdf\") as tmp:\n        temp_pdf_path = tmp.name\n        file.save(tmp)\n    \n    try:\n        # Call the PDF generation function\n        create_document_from_paper(temp_pdf_path)\n    except Exception as e:\n        return jsonify({\"error\": \"Error generating PDF\", \"details\": str(e)}), 500\n    finally:\n        os.remove(temp_pdf_path)\n    \n    pdf_file_path = \"/kaggle/working/research.pdf\"\n    # Check if the PDF file exists before sending\n    if not os.path.exists(pdf_file_path):\n        return jsonify({\"error\": \"PDF file not found\", \"details\": f\"{pdf_file_path} does not exist\"}), 500\n\n    return send_file(\"/kaggle/working/research.pdf\", as_attachment=True, download_name=\"research.pdf\")\n\n@app.route('/generate_podcast', methods=['POST'])\ndef generate_podcast():\n    # Check that a file was sent in the request\n    if 'file' not in request.files:\n        return jsonify({'error': 'No file part in the request'}), 400\n    file = request.files['file']\n    if file.filename == '':\n        return jsonify({'error': 'No file selected'}), 400\n\n    # Save the uploaded PDF to a temporary file\n    with tempfile.NamedTemporaryFile(delete=False, suffix='.pdf') as tmp:\n        file.save(tmp.name)\n        tmp_path = tmp.name\n\n    try:\n        # Generate podcast script and audio from the PDF file\n        script_data, audio_np = generate_podcast_from_pdf(tmp_path)\n    except Exception as e:\n        os.remove(tmp_path)\n        return jsonify({'error': str(e)}), 500\n\n    # Remove the temporary PDF file after processing\n    os.remove(tmp_path)\n\n    # Save the numpy audio data to a file named \"podcast.wav\" in /kaggle/working/\n    audio_file_path = \"podcast.wav\"\n    sf.write(audio_file_path, audio_np, 48000, format='WAV')\n\n    # Return the saved .wav file directly\n    return send_file(\n        \"/kaggle/working/podcast.wav\",\n        as_attachment=True,\n        download_name=\"podcast.wav\"\n    )\n\n@app.route(\"/generate_comic\", methods=[\"POST\"])\ndef generate_comic_endpoint():\n    logging.info(\"POST /generate_comic\")\n    file = request.files.get(\"file\")\n    if file is None:\n        logging.error(\"No file provided for comic generation.\")\n        return jsonify({\"error\": \"No file provided\"}), 400\n\n    with tempfile.NamedTemporaryFile(delete=False, suffix=\".pdf\") as tmp:\n        temp_pdf_path = tmp.name\n        file.save(tmp)\n\n    try:\n        comic_url = generate_comic_from_pdf(temp_pdf_path)\n    except Exception as e:\n        logging.exception(\"Error generating comic:\")\n        os.remove(temp_pdf_path)\n        return jsonify({\"error\": \"Error generating comic\", \"details\": str(e)}), 500\n    finally:\n        if os.path.exists(temp_pdf_path):\n            os.remove(temp_pdf_path)\n\n    logging.info(\"Comic generation successful. URL: %s\", comic_url)\n    return jsonify({\"comic_url\": comic_url})\n\n\nif __name__ == \"__main__\":\n    app.run(host=\"0.0.0.0\",   port=8000)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-11T10:09:35.796003Z","iopub.execute_input":"2025-03-11T10:09:35.796290Z","execution_failed":"2025-03-11T11:55:44.524Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"71fd71358a40419e9aff42bc2ca1be29"}},"metadata":{}},{"name":"stdout","text":" * Serving Flask app '__main__'\n * Debug mode: off\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}